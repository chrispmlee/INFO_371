{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Build your own spam filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data set as a dataframe\n",
    "emails = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of dataframe so we do not alter the original\n",
    "emailsplit = emails.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools necessary to create a \"bag of words\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# words are already lemmatized, so we just remove punctuation\n",
    "emailsplit['text'] = emailsplit['text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)) # remove punctuation with regex\n",
    "\n",
    "# create cached object of stopwords to improve speed\n",
    "cachedStopwords = set(stopwords.words('english'))\n",
    "\n",
    "# remove stop words\n",
    "emailsplit['text'] = emailsplit.apply(lambda x: [item for item in x if item not in cachedStopwords])\n",
    "emailsplit['text'] = emailsplit['text'].str[8:] # remove \"subject\" from beginning of each email\n",
    "\n",
    "\n",
    "#========================================\n",
    "# NOTES:\n",
    "# create one preprocessing function where we can choose what processing to do. Easier to check how it affects results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and tf-idf sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "corpus = [] # list of email content, where each item in list is an email\n",
    "for i in range(len(emailsplit['text'])):\n",
    "    corpus.append(emailsplit.loc[i]['text'])\n",
    "    \n",
    "vectorizer = CountVectorizer() # create vectorizer\n",
    "\n",
    "wordmatrix = vectorizer.fit_transform(corpus) # sparse matrix where each value ij is how many times the word j occurs in email i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (term frequency-inverse document frequency), a weighting scheme for words.\n",
    "# The weight increases when a word occurs many times in a small number of documents, leading to increased discriminatory power of the word. \n",
    "# The weight decreases when the word occurrs infrequently, or occurs in a large number of documents. \n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(wordmatrix) # create transformer based on vocab in train set\n",
    "tfidf = tfidf_transformer.transform(wordmatrix) # calculate tf-idf of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra variables\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# count dollar sign punctuation\n",
    "count_dollar = lambda l1: sum([1 for x in l1 if x in set('$')])\n",
    "emailsplit = emailsplit.assign(dollar = emails['text'].apply(count_dollar))\n",
    "\n",
    "tfidf_ex = hstack((tfidf ,np.array(emailsplit[['dollar']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse matrix\n",
    "tfidf_ex = tfidf_ex.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_ex, emailsplit['spam'], test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9671618451915559\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "# Tuning alpha\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alphas = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "model3 = MultinomialNB()\n",
    "grid3 = GridSearchCV(estimator=model3, param_grid=dict(alpha=alphas), return_train_score=True)\n",
    "grid3.fit(X_train, y_train)\n",
    "print(grid3.best_score_)\n",
    "print(grid3.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9731566495621049"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time1 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf1 = MultinomialNB(alpha=grid3.best_estimator_.alpha)\n",
    "\n",
    "# Train model\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "time1 = str(round(time.time() - start_time1, 2))\n",
    "print(time1 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf1, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921813917122753\n",
      "1.0\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# Tuning C and tol\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "grid2 = [{'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "         'tol': [0.0001, 0.001, 0.01, 0.1, 1.0]}]\n",
    "model2 = LinearSVC()\n",
    "gridcv2 = GridSearchCV(estimator=model2, param_grid=grid2, return_train_score=True)\n",
    "gridcv2.fit(X_train, y_train)\n",
    "print(gridcv2.best_score_)\n",
    "print(gridcv2.best_estimator_.C)\n",
    "print(gridcv2.best_estimator_.tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time2 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf2 = LinearSVC(C=gridcv2.best_estimator_.C, \n",
    "                 tol=gridcv2.best_estimator_.tol,\n",
    "                 random_state=0)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "time2 = str(round(time.time() - start_time2, 2))\n",
    "print(time2 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf2, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9728954912692207\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# Tuning for max_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "grid1 = [{'max_features' : ['auto', 'sqrt', 'log2', 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
    "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]}]\n",
    "\n",
    "model1 = RandomForestClassifier()\n",
    "gridcv1 = GridSearchCV(estimator=model1, param_grid=grid1, return_train_score=True)\n",
    "gridcv1.fit(X_train, y_train)\n",
    "print(gridcv1.best_score_)\n",
    "print(gridcv1.best_estimator_.max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.62 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9726304123150566"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time3 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf3 = RandomForestClassifier(max_features = gridcv1.best_estimator_.max_features, \n",
    "                              random_state=0)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "time3 = str(round(time.time() - start_time3, 2))\n",
    "\n",
    "print(time3 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf3, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Lasso Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9913995308835027\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "# Tuning C, the inverse of the regularization parameter\n",
    "from sklearn import linear_model\n",
    "\n",
    "grid4 = [{'C' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10,100,1000,10000]}]\n",
    "\n",
    "model4 = linear_model.LogisticRegression()\n",
    "gridcv4 = GridSearchCV(estimator=model4, param_grid=grid4, return_train_score=True)\n",
    "gridcv4.fit(X_train, y_train)\n",
    "print(gridcv4.best_score_)\n",
    "print(gridcv4.best_estimator_.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.994266073759791"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time4 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf4 = linear_model.LogisticRegression(C = gridcv4.best_estimator_.C, random_state=0)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "time4 = str(round(time.time() - start_time4, 2))\n",
    "\n",
    "print(time4 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf4, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "compute_rmse\n",
    "\n",
    "Given two arrays, one of actual values and one of predicted values,\n",
    "compute the Roote Mean Squared Error\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "predictions : array\n",
    "    Array of numerical values corresponding to predictions for each of the N observations\n",
    "\n",
    "yvalues : array\n",
    "    Array of numerical values corresponding to the actual values for each of the N observations\n",
    "\n",
    "Returns\n",
    "-------\n",
    "rmse : int\n",
    "    Root Mean Squared Error of the prediction\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> print compute_rmse((2,2,3),(0,2,6)\n",
    "2.08\n",
    "\"\"\"\n",
    "def compute_rmse(predictions, yvalues):\n",
    "    sigma = np.sum(np.square(np.array(predictions)-np.array(yvalues)))\n",
    "    rmse = np.sqrt(sigma/len (predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16097271743639588"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Naive Bayes, calculate accuracy using RMSE\n",
    "clf1_pred = clf1.predict(X_test)\n",
    "compute_rmse(clf1_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10284169677447053"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Support Vector Machine, calculate accuracy using RMSE\n",
    "clf2_pred = clf2.predict(X_test)\n",
    "compute_rmse(clf2_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19376860378775274"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Random Forest, calculate accuracy using RMSE\n",
    "clf3_pred = clf3.predict(X_test)\n",
    "compute_rmse(clf3_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09481535954756656"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Lasso, calculate accuracy using RMSE\n",
    "clf4_pred = clf4.predict(X_test)\n",
    "compute_rmse(clf4_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "ROC/AUC, confusion matrix, precision vs. recall, speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Specificity vs Sensitivity\n",
      "Specificity: 0.9965588437715073\n",
      "Sensitivity: 0.8995433789954338\n",
      "\n",
      "Support Vector Machine: Specificity vs Sensitivity\n",
      "Specificity: 0.9965588437715073\n",
      "Sensitivity: 0.9657534246575342\n",
      "\n",
      "Random Forest: Specificity vs Sensitivity\n",
      "Specificity: 0.9683413626978665\n",
      "Sensitivity: 0.9429223744292238\n",
      "\n",
      "Lasso Regularized Logistic Regression: Specificity vs Sensitivity\n",
      "Specificity: 0.9965588437715073\n",
      "Sensitivity: 0.9726027397260274\n",
      "\n",
      "Naive Bayes: Time to Train\n",
      "0.02 seconds\n",
      "\n",
      "Support Vector Machine: Time to Train\n",
      "0.72 seconds\n",
      "\n",
      "Random Forest: Time to Train\n",
      "3.62 seconds\n",
      "\n",
      "Lasso Regularized Logistic Regression: Time to Train\n",
      "0.16 seconds\n",
      "\n",
      "Naive Bayes: Confusion Matrix\n",
      "[[1448    5]\n",
      " [  44  394]]\n",
      "\n",
      "Support Vector Machine: Confusion Matrix\n",
      "[[1448    5]\n",
      " [  15  423]]\n",
      "\n",
      "Random Forest: Confusion Matrix\n",
      "[[1407   46]\n",
      " [  25  413]]\n",
      "\n",
      "Lasso Regularized Logistic Regression: Confusion Matrix\n",
      "[[1448    5]\n",
      " [  12  426]]\n",
      "\n",
      "Naive Bayes: ROC/AUC\n",
      "False Positive Rate: [0.         0.00344116 1.        ]\n",
      "True Positive Rate: [0.         0.89954338 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.9480511113834705\n",
      "\n",
      "Support Vector Machine: ROC/AUC\n",
      "False Positive Rate: [0.         0.00344116 1.        ]\n",
      "True Positive Rate: [0.         0.96575342 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.9811561342145207\n",
      "\n",
      "Random Forest: ROC/AUC\n",
      "False Positive Rate: [0.         0.03165864 1.        ]\n",
      "True Positive Rate: [0.         0.94292237 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.9556318685635452\n",
      "\n",
      "Lasso Regularized Logistic Regression: Confusion Matrix: ROC/AUC\n",
      "False Positive Rate: [0.         0.00344116 1.        ]\n",
      "True Positive Rate: [0.         0.97260274 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.9845807917487673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Specificity, Sensitivity\n",
    "def specificity(y_test, clf_pred):\n",
    "    spec = np.sum((y_test == 0) & (clf_pred == 0)) / (np.sum((y_test == 0) & (clf_pred == 0)) + np.sum((y_test == 0) & (clf_pred == 1)))\n",
    "    return spec\n",
    "\n",
    "def sensitivity(y_test, clf_pred):\n",
    "    sens = np.sum((y_test == 1) & (clf_pred == 1)) / (np.sum((y_test == 1) & (clf_pred == 1)) + np.sum((y_test == 1) & (clf_pred == 0)))\n",
    "    return sens\n",
    "\n",
    "print ('Naive Bayes: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf1_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf1_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Support Vector Machine: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf2_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf2_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Random Forest: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf3_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf3_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Lasso Regularized Logistic Regression: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf4_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf4_pred)))\n",
    "print('')\n",
    "\n",
    "# Time\n",
    "print ('Naive Bayes: Time to Train')\n",
    "print(time1 + \" seconds\")\n",
    "print()\n",
    "print ('Support Vector Machine: Time to Train')\n",
    "print(time2 + \" seconds\")\n",
    "print()\n",
    "print ('Random Forest: Time to Train')\n",
    "print(time3 + \" seconds\")\n",
    "print()\n",
    "print ('Lasso Regularized Logistic Regression: Time to Train')\n",
    "print(time4 + \" seconds\")\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print ('Naive Bayes: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf1_pred))\n",
    "print()\n",
    "\n",
    "print ('Support Vector Machine: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf2_pred))\n",
    "print()\n",
    "\n",
    "print ('Random Forest: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf3_pred))\n",
    "print()\n",
    "\n",
    "print ('Lasso Regularized Logistic Regression: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf4_pred))\n",
    "print()\n",
    "\n",
    "# ROC/AUC\n",
    "print ('Naive Bayes: ROC/AUC')\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, clf1_pred)\n",
    "print('False Positive Rate: ' + str(fpr1))\n",
    "print('True Positive Rate: ' + str(tpr1))\n",
    "print('Thresholds: '+ str(thresholds1))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf1_pred)))\n",
    "print()\n",
    "\n",
    "print ('Support Vector Machine: ROC/AUC')\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(y_test, clf2_pred)\n",
    "print('False Positive Rate: ' + str(fpr2))\n",
    "print('True Positive Rate: ' + str(tpr2))\n",
    "print('Thresholds: '+ str(thresholds2))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf2_pred)))\n",
    "print()\n",
    "\n",
    "print ('Random Forest: ROC/AUC')\n",
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(y_test, clf3_pred)\n",
    "print('False Positive Rate: ' + str(fpr3))\n",
    "print('True Positive Rate: ' + str(tpr3))\n",
    "print('Thresholds: '+ str(thresholds3))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf3_pred)))\n",
    "print()\n",
    "\n",
    "print ('Lasso Regularized Logistic Regression: Confusion Matrix: ROC/AUC')\n",
    "fpr4, tpr4, thresholds4 = metrics.roc_curve(y_test, clf4_pred)\n",
    "print('False Positive Rate: ' + str(fpr4))\n",
    "print('True Positive Rate: ' + str(tpr4))\n",
    "print('Thresholds: '+ str(thresholds4))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf4_pred)))\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
