{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Build your own spam filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data set as a dataframe\n",
    "emails = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of dataframe so we do not alter the original\n",
    "emailsplit = emails.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools necessary to create a \"bag of words\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# words are already lemmatized, so we just remove punctuation\n",
    "emailsplit['text'] = emailsplit['text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)) # remove punctuation with regex\n",
    "\n",
    "# create cached object of stopwords to improve speed\n",
    "cachedStopwords = set(stopwords.words('english'))\n",
    "\n",
    "# remove stop words\n",
    "emailsplit['text'] = emailsplit.apply(lambda x: [item for item in x if item not in cachedStopwords])\n",
    "emailsplit['text'] = emailsplit['text'].str[8:] # remove \"subject\" from beginning of each email\n",
    "\n",
    "\n",
    "#========================================\n",
    "# NOTES:\n",
    "# create one preprocessing function where we can choose what processing to do. Easier to check how it affects results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and tf-idf sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "corpus = [] # list of email content, where each item in list is an email\n",
    "for i in range(len(emailsplit['text'])):\n",
    "    corpus.append(emailsplit.loc[i]['text'])\n",
    "    \n",
    "vectorizer = CountVectorizer() # create vectorizer\n",
    "\n",
    "wordmatrix = vectorizer.fit_transform(corpus) # sparse matrix where each value ij is how many times the word j occurs in email i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (term frequency-inverse document frequency), a weighting scheme for words.\n",
    "# The weight increases when a word occurs many times in a small number of documents, leading to increased discriminatory power of the word. \n",
    "# The weight decreases when the word occurrs infrequently, or occurs in a large number of documents. \n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(wordmatrix) # create transformer based on vocab in train set\n",
    "tfidf = tfidf_transformer.transform(wordmatrix) # calculate tf-idf of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra variables\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# count dollar sign punctuation\n",
    "count_dollar = lambda l1: sum([1 for x in l1 if x in set('$')])\n",
    "emailsplit = emailsplit.assign(dollar = emails['text'].apply(count_dollar))\n",
    "\n",
    "tfidf_ex = hstack((tfidf ,np.array(emailsplit[['dollar']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse matrix\n",
    "tfidf_ex = tfidf_ex.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_ex, emailsplit['spam'], test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9658587438102685\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "# Tuning alpha\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alphas = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "model3 = MultinomialNB()\n",
    "grid3 = GridSearchCV(estimator=model3, param_grid=dict(alpha=alphas), return_train_score=True)\n",
    "grid3.fit(X_train, y_train)\n",
    "print(grid3.best_score_)\n",
    "print(grid3.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97523859856962"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time1 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf1 = MultinomialNB(alpha=grid3.best_estimator_.alpha)\n",
    "\n",
    "# Train model\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "time1 = str(round(time.time() - start_time1, 2))\n",
    "print(time1 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf1, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9903570497784727\n",
      "1.0\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# Tuning C and tol\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "grid2 = [{'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "         'tol': [0.0001, 0.001, 0.01, 0.1, 1.0]}]\n",
    "model2 = LinearSVC()\n",
    "gridcv2 = GridSearchCV(estimator=model2, param_grid=grid2, return_train_score=True)\n",
    "gridcv2.fit(X_train, y_train)\n",
    "print(gridcv2.best_score_)\n",
    "print(gridcv2.best_estimator_.C)\n",
    "print(gridcv2.best_estimator_.tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9924417760220632"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time2 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf2 = LinearSVC(C=gridcv2.best_estimator_.C, \n",
    "                 tol=gridcv2.best_estimator_.tol,\n",
    "                 random_state=0)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "time2 = str(round(time.time() - start_time2, 2))\n",
    "print(time2 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf2, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Tuning for max_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "grid1 = [{'max_features' : ['auto', 'sqrt', 'log2', 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
    "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]}]\n",
    "\n",
    "model1 = RandomForestClassifier()\n",
    "gridcv1 = GridSearchCV(estimator=model1, param_grid=grid1, return_train_score=True)\n",
    "gridcv1.fit(X_train, y_train)\n",
    "print(gridcv1.best_score_)\n",
    "print(gridcv1.best_estimator_.max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time3 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf3 = RandomForestClassifier(max_features = gridcv1.best_estimator_.max_features, \n",
    "                              random_state=0)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "time3 = str(round(time.time() - start_time3, 2))\n",
    "\n",
    "print(time3 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf3, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Ridge Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "# Tuning C, the inverse of the regularization parameter\n",
    "from sklearn import linear_model\n",
    "\n",
    "grid4 = [{'C' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10,100,1000,10000]}]\n",
    "\n",
    "model4 = linear_model.LogisticRegression()\n",
    "gridcv4 = GridSearchCV(estimator=model4, param_grid=grid4, return_train_score=True)\n",
    "gridcv4.fit(X_train, y_train)\n",
    "print(gridcv4.best_score_)\n",
    "print(gridcv4.best_estimator_.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of how long it takes to fit/train model\n",
    "start_time4 = time.time()\n",
    "\n",
    "# Create model with best estimators from grid search\n",
    "clf4 = linear_model.LogisticRegression(C = gridcv4.best_estimator_.C, random_state=0)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "time4 = str(round(time.time() - start_time4, 2))\n",
    "\n",
    "print(time4 + \" seconds\")\n",
    "\n",
    "# Perform k-fold cross-validation on training data to test accuracy of the model\n",
    "cross_val_score(clf4, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "compute_rmse\n",
    "\n",
    "Given two arrays, one of actual values and one of predicted values,\n",
    "compute the Roote Mean Squared Error\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "predictions : array\n",
    "    Array of numerical values corresponding to predictions for each of the N observations\n",
    "\n",
    "yvalues : array\n",
    "    Array of numerical values corresponding to the actual values for each of the N observations\n",
    "\n",
    "Returns\n",
    "-------\n",
    "rmse : int\n",
    "    Root Mean Squared Error of the prediction\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> print compute_rmse((2,2,3),(0,2,6)\n",
    "2.08\n",
    "\"\"\"\n",
    "def compute_rmse(predictions, yvalues):\n",
    "    sigma = np.sum(np.square(np.array(predictions)-np.array(yvalues)))\n",
    "    rmse = np.sqrt(sigma/len (predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15765333583312638"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Naive Bayes, calculate accuracy using RMSE\n",
    "clf1_pred = clf1.predict(X_test)\n",
    "compute_rmse(clf1_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07626944360291793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Support Vector Machine, calculate accuracy using RMSE\n",
    "clf2_pred = clf2.predict(X_test)\n",
    "compute_rmse(clf2_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14903177731762277"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Random Forest, calculate accuracy using RMSE\n",
    "clf3_pred = clf3.predict(X_test)\n",
    "compute_rmse(clf3_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07626944360291793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data using Ridge, calculate accuracy using RMSE\n",
    "clf4_pred = clf4.predict(X_test)\n",
    "compute_rmse(clf4_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "ROC/AUC, confusion matrix, precision vs. recall, speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary metrics to evaluate performance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Specificity vs Sensitivity\n",
      "Specificity: 0.9840940525587828\n",
      "Sensitivity: 0.946067415730337\n",
      "\n",
      "Support Vector Machine: Specificity vs Sensitivity\n",
      "Specificity: 0.9979253112033195\n",
      "Sensitivity: 0.9820224719101124\n",
      "\n",
      "Random Forest: Specificity vs Sensitivity\n",
      "Specificity: 0.9875518672199171\n",
      "Sensitivity: 0.946067415730337\n",
      "\n",
      "Lasso Regularized Logistic Regression: Specificity vs Sensitivity\n",
      "Specificity: 0.9979253112033195\n",
      "Sensitivity: 0.9820224719101124\n",
      "\n",
      "Naive Bayes: Time to Train\n",
      "0.01 seconds\n",
      "\n",
      "Support Vector Machine: Time to Train\n",
      "1.31 seconds\n",
      "\n",
      "Random Forest: Time to Train\n",
      "2.88 seconds\n",
      "\n",
      "Lasso Regularized Logistic Regression: Time to Train\n",
      "0.16 seconds\n",
      "\n",
      "Naive Bayes: Confusion Matrix\n",
      "[[1423   23]\n",
      " [  24  421]]\n",
      "\n",
      "Support Vector Machine: Confusion Matrix\n",
      "[[1443    3]\n",
      " [   8  437]]\n",
      "\n",
      "Random Forest: Confusion Matrix\n",
      "[[1428   18]\n",
      " [  24  421]]\n",
      "\n",
      "Lasso Regularized Logistic Regression: Confusion Matrix\n",
      "[[1443    3]\n",
      " [   8  437]]\n",
      "\n",
      "Naive Bayes: ROC/AUC\n",
      "False Positive Rate: [0.         0.01590595 1.        ]\n",
      "True Positive Rate: [0.         0.94606742 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.96508073414456\n",
      "\n",
      "Support Vector Machine: ROC/AUC\n",
      "False Positive Rate: [0.         0.00207469 1.        ]\n",
      "True Positive Rate: [0.         0.98202247 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.989973891556716\n",
      "\n",
      "Random Forest: ROC/AUC\n",
      "False Positive Rate: [0.         0.01244813 1.        ]\n",
      "True Positive Rate: [0.         0.94606742 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.966809641475127\n",
      "\n",
      "Lasso Regularized Logistic Regression: Confusion Matrix: ROC/AUC\n",
      "False Positive Rate: [0.         0.00207469 1.        ]\n",
      "True Positive Rate: [0.         0.98202247 1.        ]\n",
      "Thresholds: [2 1 0]\n",
      "AUC: 0.989973891556716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specificity, Sensitivity\n",
    "def specificity(y_test, clf_pred):\n",
    "    spec = np.sum((y_test == 0) & (clf_pred == 0)) / (np.sum((y_test == 0) & (clf_pred == 0)) + np.sum((y_test == 0) & (clf_pred == 1)))\n",
    "    return spec\n",
    "\n",
    "def sensitivity(y_test, clf_pred):\n",
    "    sens = np.sum((y_test == 1) & (clf_pred == 1)) / (np.sum((y_test == 1) & (clf_pred == 1)) + np.sum((y_test == 1) & (clf_pred == 0)))\n",
    "    return sens\n",
    "\n",
    "print ('Naive Bayes: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf1_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf1_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Support Vector Machine: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf2_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf2_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Random Forest: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf3_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf3_pred)))\n",
    "print('')\n",
    "\n",
    "print ('Ridge Regularized Logistic Regression: Specificity vs Sensitivity')\n",
    "print ('Specificity: ' + str(specificity(y_test, clf4_pred)))\n",
    "print ('Sensitivity: ' + str(sensitivity(y_test, clf4_pred)))\n",
    "print('')\n",
    "\n",
    "# Time to train model\n",
    "print ('Naive Bayes: Time to Train')\n",
    "print(time1 + \" seconds\")\n",
    "print()\n",
    "print ('Support Vector Machine: Time to Train')\n",
    "print(time2 + \" seconds\")\n",
    "print()\n",
    "print ('Random Forest: Time to Train')\n",
    "print(time3 + \" seconds\")\n",
    "print()\n",
    "print ('Ridge Regularized Logistic Regression: Time to Train')\n",
    "print(time4 + \" seconds\")\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print ('Naive Bayes: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf1_pred))\n",
    "print()\n",
    "\n",
    "print ('Support Vector Machine: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf2_pred))\n",
    "print()\n",
    "\n",
    "print ('Random Forest: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf3_pred))\n",
    "print()\n",
    "\n",
    "print ('Ridge Regularized Logistic Regression: Confusion Matrix')\n",
    "print(confusion_matrix(y_test, clf4_pred))\n",
    "print()\n",
    "\n",
    "# ROC/AUC\n",
    "print ('Naive Bayes: ROC/AUC')\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, clf1_pred)\n",
    "print('False Positive Rate: ' + str(fpr1))\n",
    "print('True Positive Rate: ' + str(tpr1))\n",
    "print('Thresholds: '+ str(thresholds1))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf1_pred)))\n",
    "print()\n",
    "\n",
    "print ('Support Vector Machine: ROC/AUC')\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(y_test, clf2_pred)\n",
    "print('False Positive Rate: ' + str(fpr2))\n",
    "print('True Positive Rate: ' + str(tpr2))\n",
    "print('Thresholds: '+ str(thresholds2))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf2_pred)))\n",
    "print()\n",
    "\n",
    "print ('Random Forest: ROC/AUC')\n",
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(y_test, clf3_pred)\n",
    "print('False Positive Rate: ' + str(fpr3))\n",
    "print('True Positive Rate: ' + str(tpr3))\n",
    "print('Thresholds: '+ str(thresholds3))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf3_pred)))\n",
    "print()\n",
    "\n",
    "print ('Ridge Regularized Logistic Regression: Confusion Matrix: ROC/AUC')\n",
    "fpr4, tpr4, thresholds4 = metrics.roc_curve(y_test, clf4_pred)\n",
    "print('False Positive Rate: ' + str(fpr4))\n",
    "print('True Positive Rate: ' + str(tpr4))\n",
    "print('Thresholds: '+ str(thresholds4))\n",
    "print('AUC: ' + str(roc_auc_score(y_test, clf4_pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>FPR</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>TPR</th>\n",
       "      <th>Time</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.31</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.946</td>\n",
       "      <td>2.88</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.16</td>\n",
       "      <td>Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy    FPR   RMSE    TPR  Time model\n",
       "0     0.975  0.016  0.158  0.946  0.01    NB\n",
       "1     0.994  0.002  0.076  0.982  1.31   SVM\n",
       "2     0.978  0.012  0.149  0.946  2.88    RF\n",
       "3     0.994  0.002  0.076  0.982  0.16   Log"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary table of performance metric results\n",
    "rocdf = {'model':['NB','SVM','RF', 'Log'], \n",
    "         'TPR':[tpr1[1], tpr2[1], tpr3[1], tpr4[1]],\n",
    "        'FPR':[fpr1[1], fpr2[1], fpr3[1], fpr4[1]],\n",
    "        'Time':[time1, time2, time3, time4],\n",
    "        'RMSE':[compute_rmse(clf1_pred, y_test),\n",
    "               compute_rmse(clf2_pred, y_test),\n",
    "               compute_rmse(clf3_pred, y_test),\n",
    "               compute_rmse(clf4_pred, y_test)],\n",
    "        'Accuracy':[metrics.accuracy_score(y_test,clf1_pred),\n",
    "                   metrics.accuracy_score(y_test,clf2_pred),\n",
    "                   metrics.accuracy_score(y_test,clf3_pred),\n",
    "                   metrics.accuracy_score(y_test,clf4_pred)]}\n",
    "\n",
    "rocdf = pd.DataFrame(rocdf)\n",
    "rocdf.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
