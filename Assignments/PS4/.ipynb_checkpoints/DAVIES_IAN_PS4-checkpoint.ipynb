{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Build your own spam filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "# Create word dictionary\n",
    "# Extract features\n",
    "# Train classifier\n",
    "\n",
    "# ideas:\n",
    "# use tf-idf to weight features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smaller dataset to play with\n",
    "emailsplit = emails.copy()\n",
    "# emailsplit = emailsplit.loc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# words are already lemmatized, so we just remove punctuation\n",
    "emailsplit['text'] = emailsplit['text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)) # remove punctuation with regex\n",
    "\n",
    "# create cached object of stopwords to improve speed\n",
    "cachedStopwords = set(stopwords.words('english'))\n",
    "\n",
    "# remove stop words\n",
    "emailsplit['text'] = emailsplit.apply(lambda x: [item for item in x if item not in cachedStopwords])\n",
    "emailsplit['text'] = emailsplit['text'].str[8:] # remove \"subject\" from beginning of each email\n",
    "\n",
    "# remove numbers (alternatively, could convert to a single constant like a punctuation symbol to preserve some info)\n",
    "# emailsplit['text'] = emailsplit['text'].apply(lambda x: [item for item in x if item not in set())])\n",
    "\n",
    "#========================================\n",
    "# NOTES:\n",
    "# create one preprocessing function where we can choose what processing to do. Easier to check how it affects results.\n",
    "# does feature_extraction already do the above? may be redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words and tf-idf sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "corpus = [] # list of email content, where each item in list is an email\n",
    "for i in range(len(emailsplit['text'])):\n",
    "    corpus.append(emailsplit.loc[i]['text'])\n",
    "    \n",
    "vectorizer = CountVectorizer() # create vectorizer\n",
    "\n",
    "wordmatrix = vectorizer.fit_transform(corpus) # sparse matrix where each value ij is how many times the word j occurs in email i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (term frequency-inverse document frequency), a weighting scheme for words.\n",
    "# The weight increases when a word occurs many times in a small number of documents, leading to increased discriminatory power of the word. \n",
    "# The weight decreases when the word occurrs infrequently, or occurs in a large number of documents. \n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(wordmatrix) # create transformer based on vocab in train set\n",
    "tfidf = tfidf_transformer.transform(wordmatrix) # calculate tf-idf of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model 2: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9582958797258259"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, emailsplit['spam'], test_size=0.33)\n",
    "\n",
    "clf1 = tree.DecisionTreeClassifier()\n",
    "clf1 = clf1.fit(X_train, y_train)\n",
    "\n",
    "# cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(clf1, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946052273716275"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, emailsplit['spam'], test_size=0.33)\n",
    "\n",
    "clf2 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf2 = clf2.fit(X_train, y_train)\n",
    "\n",
    "# cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(clf1, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9465728433672139\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# tuning for max_depth\n",
    "\n",
    "# Create grid search\n",
    "# def make_grid_search(classifier, grid, train_features, train_outcome, test_features, test_outcome):\n",
    "#     from sklearn.pipeline import make_pipeline\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "#     param_grid = grid\n",
    "#     model = classifier\n",
    "#     pipe = make_pipeline(scaler, model)\n",
    "    \n",
    "#     from sklearn.model_selection import GridSearchCV\n",
    "#     grid = GridSearchCV(pipe, param_grid)\n",
    "#     search_model = grid.fit(train_features, train_outcome)\n",
    "#     return search_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# n_estimators = np.array(np.arange(50,70,1)) # the optimal parameters always seem to be the bigger ones\n",
    "# model = RandomForestClassifier()\n",
    "# grid = GridSearchCV(estimator=model, param_grid=dict(n_estimators=n_estimators), return_train_score=True)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_estimator_.n_estimators)\n",
    "\n",
    "# SVM\n",
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics\n",
    "\n",
    "ROC/AUC, confusion matrix, precision vs. recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating extra variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats .. any difference in length, word diversity, number of punctuation symbols?\n",
    "\n",
    "# might not even need these ... instead of counting dollar signs, just doing word/punct frequency in naive bayes might be more effective\n",
    "emails_copy = pd.read_csv(\"emails.csv\")\n",
    "\n",
    "# punctuation count\n",
    "count_punct = lambda l1: sum([1 for x in l1 if x in set(string.punctuation)])\n",
    "emails_copy = emails_copy.assign(punct = emails_copy['text'].apply(count_punct))\n",
    "\n",
    "# message length\n",
    "emails_copy = emails_copy.assign(length = emails_copy['text'].apply(len))\n",
    "\n",
    "# count dollar sign punctuation\n",
    "count_dollar = lambda l1: sum([1 for x in l1 if x in set('$')])\n",
    "emails_copy = emails_copy.assign(dollar = emails_copy['text'].apply(count_dollar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dollar = lambda l1: sum([1 for x in l1 if x in set('$')])\n",
    "emails_copy = emails_copy.assign(dollar = emails_copy['text'].apply(count_dollar))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
